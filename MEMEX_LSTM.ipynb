{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Deep Learning for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Kyle Hundman (Kyle.A.Hundman@jpl.nasa.gov)<br>\n",
    "**Role:** Data Scientist, NASA Jet Propulsion Laboratory <br>\n",
    "**Work Sponsored By:** DARPA MEMEX Program <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A character-level deep learning model used to classify online escort ads as 'warranting human-trafficking investigation' or not. Code was written as part of DARPA Memex summer hack (2016). Model design is largely derived from Stathis Vafeias' blog: https://offbit.github.io/how-to-read/ with inspiration from various other sources:\n",
    "\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- https://arxiv.org/pdf/1502.01710v5.pdf\n",
    "- https://offbit.github.io/how-to-read/\n",
    "- http://emnlp2014.org/papers/pdf/EMNLP2014181.pdf\n",
    "- http://arxiv.org/pdf/1506.02078v2.pdf\n",
    "\n",
    "A character level model was appealing for this problem because of the inherent disorganization of crawled web content, which was especially true in the realm of escort ads. By allowing the model to learn word permutations at the character level, very little cleaning (stemming, spell-checking, tokenization) is necessary - we can let the model learn infer vocabulary and meaning. In general, character level models are also attractive for textual data because of lower dimensionality, (40-100 character encodings vs. an arbitratily large vocabulary size). \n",
    "\n",
    "## Data\n",
    "\n",
    "Talk about clustering (why clustered), JSON structure\n",
    "\n",
    "## Organization (ToDo)\n",
    "\n",
    "Setup<br>\n",
    "Data Preprocessing <br>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "- **Python 3**\n",
    "- Using **Keras** with **Tensorflow (0.9)** backend\n",
    "- to switch between theano and tensorflow change name in config file: ```~/.keras/keras.json```\n",
    "- Ran on Macbook Pro NVIDIA GPU using Cuda and CuDNN (http://blog.wenhaolee.com/run-keras-on-mac-os-with-gpu/)\n",
    "- Outputs from all intermediate preprocessing steps are written to **```model_dir```** directory\n",
    "- Specify name of saved model weights as **```checkpoint```** to resume training prior model\n",
    "    - model weights should be located in **```checkpoints/```** dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, Dropout, MaxPooling1D, Convolution1D\n",
    "from keras.layers import LSTM, Lambda, merge\n",
    "from keras.layers import Embedding, TimeDistributed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from keras import backend as K\n",
    "import keras.callbacks\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import ujson\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "# ToDos\n",
    "# ===================\n",
    "# Create model_dir automatically if not supplied (use time)\n",
    "# If model_dir doesn't exist create it\n",
    "# If checkpoints folder doens't exist in dir, create it\n",
    "\n",
    "model_dir = \"7-30-16\"\n",
    "checkpoint = \"__main__.00-0.38.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String cleaning\n",
    "- html and newline / carriage return characters already stripped from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "### CONSIDER NOT CLEANING STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build cluster assignment dictionaries\n",
    "- Training test split done at cluster level to ensure minimal overlap between phone numbers and similar ads\n",
    "- When shuffling clusters for training/test split, this allows for easy retrieval of ads (by ID) for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_id_lookup(infile, outfile, limit=10000000):\n",
    "    \"\"\"\n",
    "    Output dictionary of cluster, doc_id assignments\n",
    "    :param infile: raw data (each line is json doc representing an ad)\n",
    "    :param outfile: {\"<cluster_id1>\" : [\"<doc_id1>\", \"<doc_id2>\", ...], \"<cluster_id2>\" : [\"<doc_id3>\", \"<doc_id4>\", ...]}\n",
    "    \"\"\"\n",
    "    clusters = {}\n",
    "    with open(os.path.join(model_dir, outfile), \"w\") as out:\n",
    "        with open(os.path.join(model_dir,infile)) as f:\n",
    "            for idx,ad in enumerate(f):\n",
    "                if idx < limit:\n",
    "                    ad = json.loads(ad)\n",
    "                    if ad[\"cluster_id\"] in clusters:\n",
    "                        clusters[ad[\"cluster_id\"]].append(ad[\"doc_id\"])\n",
    "                    else:\n",
    "                        clusters[ad[\"cluster_id\"]] = [ad[\"doc_id\"]]\n",
    "                else:\n",
    "                    break\n",
    "            out.write(json.dumps(clusters, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_id_lookup(\"cp1_positives.json\", \"training_pos_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_id_lookup(\"CP1_negatives_all_clusters.json\", \"training_neg_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Sample Cluster IDs from positive and negative training\n",
    "- Sampling done separately for positive and negative ads because they were clustered and processed at different times, resulting in overlapping cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_clusters(file):\n",
    "    \"\"\"\n",
    "    Read in clusters and doc_ids generated from build_id_lookup function\n",
    "    \"\"\"  \n",
    "    with open(os.path.join(model_dir, file)) as f:\n",
    "        clusters = eval(f.read())\n",
    "        return clusters\n",
    "\n",
    "pos_clusters_lookup = read_clusters(\"training_pos_ids\")\n",
    "neg_clusters_lookup = read_clusters(\"training_neg_ids\")\n",
    "\n",
    "pos_indices = np.arange(len(pos_clusters_lookup.keys()))\n",
    "neg_indices = np.arange(len(neg_clusters_lookup.keys()))\n",
    "\n",
    "np.random.shuffle(pos_indices)\n",
    "np.random.shuffle(neg_indices)\n",
    "\n",
    "pos_clusters = np.array(list(pos_clusters_lookup.keys()))[pos_indices]\n",
    "neg_clusters = np.array(list(neg_clusters_lookup.keys()))[neg_indices]\n",
    "\n",
    "pos_train_clusters = pos_clusters[:int(round(len(pos_clusters)*.7, 0))]\n",
    "neg_train_clusters = neg_clusters[:int(round(len(neg_clusters)*.7, 0))]\n",
    "\n",
    "pos_test_clusters = pos_clusters[int(round(len(pos_clusters)*.7, 0)):]\n",
    "neg_test_clusters = neg_clusters[int(round(len(neg_clusters)*.7, 0)):]\n",
    "\n",
    "with open(os.path.join(model_dir, \"cluster_splits\"), \"w\") as out:\n",
    "    splits = {}\n",
    "    splits[\"pos_train\"] = list(pos_train_clusters)\n",
    "    splits[\"neg_train\"] = list(neg_train_clusters)\n",
    "    splits[\"pos_test\"] = list(pos_test_clusters)\n",
    "    splits[\"neg_test\"] = list(neg_test_clusters)\n",
    "    out.write(json.dumps(splits, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve ad text based on cluster train/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cluster_ad_text(json_file, outfiles, cluster_id_lists):\n",
    "    \"\"\"\n",
    "    Get all ads associated with a set of clusters (train or test set), write to file\n",
    "    :param json_file: original ad data\n",
    "    TODO: allowing for loading in of previous cluster splits avoid resamping clusters \n",
    "    if re-running steps from here down \n",
    "    \"\"\"\n",
    "    with open(os.path.join(model_dir, outfiles[0]), \"w\") as out0:\n",
    "        with open(os.path.join(model_dir, outfiles[1]), \"w\") as out1:\n",
    "            with open(os.path.join(model_dir, json_file), \"r\") as f:\n",
    "                \n",
    "                for ad in f.readlines():\n",
    "                    ad = ujson.loads(ad)\n",
    "                    if ad[\"cluster_id\"] in cluster_id_lists[0]:\n",
    "                        out0.write(re.sub(\"(\\r|\\n|\\t)\", \" \", clean_str(ad[\"extracted_text\"])) + \"\\n\")\n",
    "                    elif ad[\"cluster_id\"] in cluster_id_lists[1]:\n",
    "                        out1.write(re.sub(\"(\\r|\\n|\\t)\", \" \", clean_str(ad[\"extracted_text\"])) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_cluster_ad_text(\"cp1_positives.json\", [\"pos_training\", \"pos_test\"], [pos_train_clusters, pos_test_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_cluster_ad_text(\"cp1_negatives_all_clusters.json\", [\"neg_training\", \"neg_test\"], [neg_train_clusters, neg_test_clusters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training/test ads, split sentences, assign labels\n",
    "- Split ads into quasi-sentences (help speed up processing) -> this could be a lot better\n",
    "- Join together docs (ads) and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_docs, test_docs = [], []\n",
    "train_sentences, test_sentences = [], []\n",
    "train_classes, test_classes = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_lists(file, docs, sentences, classes=None, label=None):\n",
    "    with open(os.path.join(model_dir, file)) as f:\n",
    "        ads = f.readlines()\n",
    "        if label == None:\n",
    "            for ad in ads:\n",
    "                ad = eval(ad)\n",
    "                sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|\\))\\s', ad[1])\n",
    "                sentences = [sent.lower() for sent in sentences]\n",
    "                docs.append(sentences)\n",
    "        else:\n",
    "            _class = [label] * len(ads)\n",
    "\n",
    "            for ad, label in zip(ads, _class):\n",
    "                sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|\\))\\s', ad)\n",
    "                sentences = [sent.lower() for sent in sentences]\n",
    "                docs.append(sentences)\n",
    "                classes.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_lists(\"pos_training\", train_docs, train_sentences, train_classes, 1)\n",
    "build_lists(\"neg_training\", train_docs, train_sentences, train_classes, 0)\n",
    "\n",
    "build_lists(\"pos_test\", test_docs, test_sentences, test_classes, 1)\n",
    "build_lists(\"neg_test\", test_docs, test_sentences, test_classes, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character quantization (encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars:  46\n",
      "{\n",
      "    \"\\n\": 24,\n",
      "    \" \": 38,\n",
      "    \"!\": 25,\n",
      "    \"'\": 18,\n",
      "    \"(\": 41,\n",
      "    \")\": 29,\n",
      "    \",\": 23,\n",
      "    \"0\": 37,\n",
      "    \"1\": 39,\n",
      "    \"2\": 14,\n",
      "    \"3\": 43,\n",
      "    \"4\": 27,\n",
      "    \"5\": 16,\n",
      "    \"6\": 21,\n",
      "    \"7\": 11,\n",
      "    \"8\": 15,\n",
      "    \"9\": 33,\n",
      "    \"?\": 1,\n",
      "    \"\\\\\": 5,\n",
      "    \"`\": 30,\n",
      "    \"a\": 10,\n",
      "    \"b\": 17,\n",
      "    \"c\": 6,\n",
      "    \"d\": 2,\n",
      "    \"e\": 32,\n",
      "    \"f\": 42,\n",
      "    \"g\": 12,\n",
      "    \"h\": 31,\n",
      "    \"i\": 19,\n",
      "    \"j\": 13,\n",
      "    \"k\": 44,\n",
      "    \"l\": 4,\n",
      "    \"m\": 8,\n",
      "    \"n\": 22,\n",
      "    \"o\": 34,\n",
      "    \"p\": 20,\n",
      "    \"q\": 7,\n",
      "    \"r\": 9,\n",
      "    \"s\": 36,\n",
      "    \"t\": 35,\n",
      "    \"u\": 45,\n",
      "    \"v\": 3,\n",
      "    \"w\": 28,\n",
      "    \"x\": 40,\n",
      "    \"y\": 26,\n",
      "    \"z\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "txt = ''\n",
    "\n",
    "def get_chars(docs, txt):\n",
    "    for doc in docs:\n",
    "        for s in doc:\n",
    "            txt += s\n",
    "    return txt\n",
    "\n",
    "txt = get_chars(train_docs, txt)\n",
    "txt = get_chars(test_docs, txt)\n",
    "        \n",
    "chars = set(txt)\n",
    "print('total chars: ', len(chars))\n",
    "char_indices = dict((c,i) for i,c in enumerate(chars))\n",
    "\n",
    "print(json.dumps(char_indices, indent=4, sort_keys=True))\n",
    "\n",
    "with open(os.path.join(model_dir,\"encodings\"), \"w\") as out:\n",
    "    out.write(json.dumps(char_indices, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character quantization and input trimming\n",
    "<a src=\"https://arxiv.org/pdf/1502.01710v5.pdf\">Zhang et. al </a> makes an interesting observation that quantization is visually similar to Braille, and that humans can learn to read binary encodings of languages, suggesting this might offer legitimacy to the approach. \n",
    "\n",
    "- Sentence lengths are trimmend and sentences per ad are limited (model needs consistent input dimensions)\n",
    "- Due to the difficulty of splitting sentences with these ads (and in general), we are making the assumption that the most salient information is in the first X number of characters and first X number of sentences in each ad. Often the longer sentences or sentences toward the end of the ad include repeating locations, names, or phone numbers which we don't want the model to learn from and become biased towards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# max number of characters allowed in a sentence, any additional are thrown out\n",
    "maxlen = 512\n",
    "\n",
    "# max sentences allowed in a doc, any additional are thrown out\n",
    "max_sentences = 15\n",
    "\n",
    "def load_char_encodings():\n",
    "    with open(os.path.join(model_dir, \"encodings\"), \"r\") as f:\n",
    "        return eval(f.read())\n",
    "\n",
    "def encode_and_trim(docs, X, maxlen, max_sentences):\n",
    "    \"\"\"\n",
    "    Replace -1's in vector representation of chars with encodings in reverse order (-1s toward beginning indicate \n",
    "    character length was less than max allowed)\n",
    "    \"\"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        for j, sentence in enumerate(doc):\n",
    "            if j < max_sentences:\n",
    "                for t, char in enumerate(sentence[-maxlen:]):\n",
    "                    X[i, j, (maxlen-1-t)] = char_indices[char]\n",
    "    return X\n",
    "\n",
    "char_indices = load_char_encodings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chars in X:[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  5]\n",
      "y:1\n"
     ]
    }
   ],
   "source": [
    "# Create array for vector representation of chars (512D). Filled with -1 initially\n",
    "X_train = np.ones((len(train_docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "X_test = np.ones((len(test_docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "\n",
    "# create array of class labels\n",
    "y_train = np.array(train_classes)\n",
    "y_test = np.array(test_classes)\n",
    "\n",
    "X_train = encode_and_trim(train_docs, X_train, maxlen, max_sentences)\n",
    "X_test = encode_and_trim(test_docs, X_test, maxlen, max_sentences)\n",
    "\n",
    "print('Sample chars in X:{}'.format(X_train[20, 2]))\n",
    "print('y:{}'.format(y_train[12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record training history (losses and accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('acc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras / Tensorflow helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def binarize(x, sz=46):\n",
    "    \"\"\"\n",
    "    Used in creation of Lambda layer to create a one hot encoding of sentence characters on the fly. \n",
    "    x : tensor of dimensions (maximum sentence length, ) TODO\n",
    "    sz : number of unique characters in the corpus\n",
    "    tf.to_float casts a tensor to type \"float32\"\n",
    "    \"\"\"\n",
    "    one_hot = tf.one_hot(x, sz, on_value=1, off_value=0, axis=-1)\n",
    "    return tf.to_float(one_hot)\n",
    "\n",
    "\n",
    "def binarize_outshape(in_shape):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return in_shape[0], in_shape[1], 46\n",
    "\n",
    "\n",
    "def max_1d(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return K.max(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence encoder\n",
    "\n",
    "- Assign input shapes for sentences and documents\n",
    "\n",
    "- **```Lambda```** is used for evaluating a tf expression on the output of a previous layer (in this case the sentence encodings)\n",
    "\n",
    "\n",
    "- **```Convolution```** theory is that similar to how hierarchical image features are learned using convolutional NN's for computer vision, hierarchical representations of words, phrases, and sentences can be learned from text\n",
    "    - **```Dropouts```** are lower than seen in most research (usually ~0.5), could cause generalization problems (dropout prevents learning a dependence on certain features)\n",
    "    - **```Max pooling```** picks strongest local features in order to summarize patch of text and encourage sparsity. Allows for deeper models \n",
    "        - A higher **```pool_length```** reduces training time in convolution layer, but likely would have little effect in overall training time\n",
    "    - **```border_mode```** set to 'valid' means no padding is created and output is smaller than input (convolution is only computed where the input and filter fully overlap)\n",
    "        - This helps manage dimensionality passed to hidden layers (easier training), plus it doesn't really make sense to pad for textual problems\n",
    "        \n",
    "        \n",
    "- **```LSTM```** first two layers here read \"word\" sequence and positions (LSTM allows \"memories\" to be stored and activated along sequencies when needed - in this portion of the model the sequences are sentences)\n",
    "    - Nice property of LSTM (RNNs in general) is input data can be arbitrarily sized, allowing user to determine dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# max number of characters allowed in a sentence, any additional are thrown out\n",
    "maxlen = 512\n",
    "\n",
    "# max sentences allowed in a doc, any additional are thrown out\n",
    "max_sentences = 15\n",
    "\n",
    "with tf.device(\"/gpu:0\"): #using Macbook Pro NVIDIA GPU\n",
    "    filter_length = [5, 3, 3]\n",
    "    nb_filter = [196, 196, 256]\n",
    "    pool_length = 2\n",
    "\n",
    "    # document input -> 15 x 512\n",
    "    document = Input(shape=(max_sentences, maxlen), dtype='int64')\n",
    "\n",
    "    # sentence input -> 512,\n",
    "    in_sentence = Input(shape=(maxlen,), dtype='int64')\n",
    "\n",
    "    # binarize function creates a onehot encoding of each character index\n",
    "    embedded = Lambda(binarize, output_shape=binarize_outshape)(in_sentence)\n",
    "\n",
    "\n",
    "    for i in range(len(nb_filter)):\n",
    "        embedded = Convolution1D(nb_filter=nb_filter[i],\n",
    "                                filter_length=filter_length[i],\n",
    "                                border_mode='valid',\n",
    "                                activation='relu',\n",
    "                                init='glorot_normal',\n",
    "                                subsample_length=1)(embedded)\n",
    "\n",
    "        embedded = Dropout(0.1)(embedded)\n",
    "        embedded = MaxPooling1D(pool_length=pool_length)(embedded)\n",
    "\n",
    "    forward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(embedded)\n",
    "    backward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(embedded)\n",
    "\n",
    "    sent_encode = merge([forward_sent, backward_sent], mode='concat', concat_axis=-1)\n",
    "    sent_encode = Dropout(0.3)(sent_encode)\n",
    "\n",
    "    encoder = Model(input=in_sentence, output=sent_encode)\n",
    "    encoded = TimeDistributed(encoder)(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document encoder\n",
    "\n",
    "- **```LSTM```** layers here store \"memories\" across sentences for a given ad. Need less recurrent layers here because output from prior layers will be smaller than original input and less information needs to be processed (max_pooling, valid border setting, convolution)\n",
    "\n",
    "\n",
    "- **```Dense```** layers are fully connected layers where all inputs from prior layer are considered (these are appropriate and can be trained in reasonable time because inputs from prior layers are much smaller than original input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    forwards = LSTM(80, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(encoded)\n",
    "    backwards = LSTM(80, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(encoded)\n",
    "\n",
    "    merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "    output = Dropout(0.3)(merged)\n",
    "    output = Dense(128, activation='relu')(output)\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model(input=document, output=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### point to checkpoint to lodel model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    if checkpoint:\n",
    "        model.load_weights(os.path.join(model_dir, \"checkpoints\", checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model\n",
    "- 1 epoch on Mac GPU takes ~36 hrs for ~200k ads as input and evaulation on ~80k ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # file_name = os.path.basename(sys.argv[0]).split('.')[0]\n",
    "    # check_cb = keras.callbacks.ModelCheckpoint(os.path.join(model_dir, 'checkpoints/'+file_name+'.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "    #                                            monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
    "    # earlystop_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, verbose=1, mode='auto')\n",
    "    # history = LossHistory()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=10,\n",
    "    #           nb_epoch=5, shuffle=True, callbacks=[earlystop_cb,check_cb, history])\n",
    "\n",
    "    # just showing access to the history object\n",
    "    # print(history.losses)\n",
    "    # print(history.accuracies)\n",
    "\n",
    "    # WRITE MODEL STATS TO MODEL DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Quantization of chars in eval docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    def get_cluster_and_text_and_id(json_file, outfile):\n",
    "        \"\"\"\n",
    "        Get all ads associated with a set of clusters (train or test set), write to file\n",
    "        \"\"\"\n",
    "        with open(os.path.join(model_dir, outfile), \"w\") as out:\n",
    "            with open(os.path.join(model_dir, json_file), \"r\") as f:\n",
    "\n",
    "                for idx, ad in enumerate(f.readlines()):\n",
    "                    ad = ujson.loads(ad)\n",
    "                    if \"class\" in ad:\n",
    "                        out.write(str([ ad[\"doc_id\"], re.sub(\"(\\r|\\n|\\t)\", \" \", clean_str(ad[\"extracted_text\"])), ad[\"cluster_id\"], ad[\"class\"] ]) + \"\\n\")\n",
    "                    else:\n",
    "                        out.write(str([ ad[\"doc_id\"], re.sub(\"(\\r|\\n|\\t)\", \" \", clean_str(ad[\"extracted_text\"])), ad[\"cluster_id\"] ]) + \"\\n\")\n",
    "    get_cluster_and_text_and_id(\"cp1_evaluation.json\", \"actual_eval\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    eval_docs = []\n",
    "    eval_sentences = []\n",
    "    eval_class = []\n",
    "\n",
    "    build_lists(\"actual_eval\", eval_docs, eval_sentences)\n",
    "    X_eval = np.ones((len(eval_docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "    X_eval = encode_and_trim(eval_docs, X_eval, maxlen, max_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate probabilities from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    proba = model.predict(X_eval, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def join_cluster_to_probability(probs):\n",
    "    \"\"\"\n",
    "    After probability assignments are made, rejoin them with a cluster id, allowing for prediction at the cluster level\n",
    "    \"\"\"\n",
    "    cluster_probs = {}\n",
    "    with open(os.path.join(model_dir, \"actual_eval\"), \"r\") as f:\n",
    "        for i, doc in enumerate(f.readlines()):\n",
    "            doc = eval(doc)\n",
    "            cluster_id = doc[2]\n",
    "            if cluster_id in cluster_probs:\n",
    "                cluster_probs[cluster_id][\"scores\"].append(float(probs[i][0]))\n",
    "            else:\n",
    "                cluster_probs[cluster_id] = {} \n",
    "                cluster_probs[cluster_id][\"scores\"] = [float(probs[i][0])]\n",
    "\n",
    "            if len(doc) == 4:\n",
    "                cluster_probs[cluster_id][\"label\"] = doc[3]\n",
    "#                 except ValueError(\"don't have labels...\")\n",
    "\n",
    "    with open(os.path.join(model_dir, \"eval_ad_predictions_by_cluster\"), \"w\") as out:\n",
    "        out.write(json.dumps(cluster_probs, indent=4))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "join_cluster_to_probability(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate score for cluster based on individual scores of ads in that cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at distribution of scores within clusters we have, generate ROC off of some aggregation of scores\n",
    "    # Average\n",
    "    # top 10%, 25%\n",
    "    # Histograms\n",
    "def avg_aggregation():\n",
    "    \"\"\"\n",
    "    :param (int) top_n: Number of highest scores to average when scoring cluster\n",
    "    \"\"\"\n",
    "    pos_clusters, neg_clusters, pos_averages, neg_averages = [], [], [], []\n",
    "    with open(os.path.join(model_dir, \"ad_predictions_by_cluster\"), \"r\") as f:\n",
    "        lookup = eval(f.read())\n",
    "        for cluster in lookup.keys():\n",
    "            if lookup[cluster][\"label\"] == 1:\n",
    "                pos_clusters.append(cluster)\n",
    "                top_pos = sorted(lookup[cluster][\"scores\"], reverse=True)[:len(lookup[cluster])]\n",
    "                pos_averages.append( sum(top_pos) / len(top_pos) )\n",
    "            elif lookup[cluster][\"label\"] == 0:\n",
    "                neg_clusters.append(cluster)\n",
    "                top_neg = sorted(lookup[cluster][\"scores\"], reverse=True)[:len(lookup[cluster][\"scores\"])]\n",
    "                neg_averages.append( sum(top_neg) / len(top_neg) )\n",
    "    return [pos_clusters, neg_clusters, pos_averages, neg_averages]\n",
    "\n",
    "def graph_cluster_scores(clusters, agg_scores, color):\n",
    "    plt.rcParams['figure.figsize'] = (17.0, 7.0)\n",
    "    bar_width = .3\n",
    "    index = np.arange(len(clusters))\n",
    "    plt.bar(index, agg_scores, .5, color=color)\n",
    "    plt.ylim([0,1.1])\n",
    "    plt.xlim([0,len(clusters)])\n",
    "    plt.xlabel('Cluster ID')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xticks(index + bar_width, clusters, rotation=\"vertical\")\n",
    "    plt.title(r'Average Ad HT Probability per Cluster')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_n = None #just using length\n",
    "\n",
    "pos_clusters = avg_aggregation()[0]\n",
    "neg_clusters = avg_aggregation()[1]\n",
    "pos_agg_scores = avg_aggregation()[2]\n",
    "neg_agg_scores = avg_aggregation()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(len(pos_agg_scores))\n",
    "# print(sum(pos_agg_scores) / len(pos_agg_scores))\n",
    "# print(max(pos_agg_scores))\n",
    "# print(pos_agg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(len(neg_agg_scores))\n",
    "# print(sum(neg_agg_scores) / len(neg_agg_scores))\n",
    "# print(max(neg_agg_scores))\n",
    "# print(neg_agg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_agg_scores, pos_clusters = zip(*sorted(zip(pos_agg_scores, pos_clusters), reverse=True))\n",
    "graph_cluster_scores(pos_clusters, pos_agg_scores, \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_agg_scores, neg_clusters = zip(*sorted(zip(neg_agg_scores, neg_clusters), reverse=True))\n",
    "graph_cluster_scores(neg_clusters, neg_agg_scores, \"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Generate files for IST evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, \"actual_evaluation\", \"ground_truth.json\"), \"w\") as out:\n",
    "    for x in range(0, len(neg_clusters)):\n",
    "        doc = {}\n",
    "        doc[\"cluster_id\"] = neg_clusters[x]\n",
    "        doc[\"class\"] = 0\n",
    "        out.write(json.dumps(doc) + \"\\n\")\n",
    "    for y in range(0, len(pos_clusters)):\n",
    "        doc = {}\n",
    "        doc[\"cluster_id\"] = pos_clusters[y]\n",
    "        doc[\"class\"] = 1\n",
    "        out.write(json.dumps(doc) + \"\\n\")\n",
    "\n",
    "with open(os.path.join(model_dir, \"actual_evaluation\", \"submission.json\"), \"w\") as out:\n",
    "    for x in range(0, len(neg_clusters)):\n",
    "        doc = {}\n",
    "        doc[\"cluster_id\"] = neg_clusters[x]\n",
    "        doc[\"score\"] = neg_agg_scores[x]\n",
    "        out.write(json.dumps(doc) + \"\\n\")\n",
    "    for y in range(0, len(pos_clusters)):\n",
    "        doc = {}\n",
    "        doc[\"cluster_id\"] = pos_clusters[y]\n",
    "        doc[\"score\"] = pos_agg_scores[y]\n",
    "        out.write(json.dumps(doc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Happens in evaluation dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DOs\n",
    "- Finish writing about intuition and script\n",
    "    - refactor\n",
    "    - identify opportunities for experimentation \n",
    "        - fully connected layers\n",
    "        - different characters allowed\n",
    "        - lattice\n",
    "        - titles\n",
    "        - sentence splits\n",
    "        - num_chars, sentences allowed\n",
    "- Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
